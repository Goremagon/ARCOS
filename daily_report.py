import os
import datetime
import re
import shutil
import time

# --- Configuration ---
RAW_DATA_DIR = "workspace"
BRIEFING_DIR = "daily_briefings"
ARCHIVE_DIR = "workspace/archive"
OUTPUT_FILE = os.path.join(BRIEFING_DIR, f"Daily_Briefing_{datetime.date.today()}.md")

# RETENTION POLICY: How many days to keep the zip files?
# Institutional Standard = 2555 days (7 years)
# Personal Standard = 365 days
RETENTION_DAYS = 365 

def parse_report(filepath):
    """
    Reads a raw ARCOS text file and extracts key data.
    """
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    
    ticker_match = re.search(r"TICKER:\s+(.*)", content)
    signal_match = re.search(r"SIGNAL:\s+(.*)", content)
    prob_match = re.search(r"PROBABILITY:\s+Model estimates a ([\d\.]+) probability", content)
    rationale_match = re.search(r"RATIONALE:\s+(.*)", content)
    
    if ticker_match and signal_match and prob_match:
        return {
            "ticker": ticker_match.group(1).strip(),
            "signal": signal_match.group(1).strip(),
            "prob": float(prob_match.group(1)),
            "rationale": rationale_match.group(1).strip() if rationale_match else "N/A"
        }
    return None

def generate_briefing():
    # 1. Ensure output folders exist
    if not os.path.exists(BRIEFING_DIR):
        os.makedirs(BRIEFING_DIR)
    if not os.path.exists(ARCHIVE_DIR):
        os.makedirs(ARCHIVE_DIR)

    print(f"üìä [Reporter] Scanning {RAW_DATA_DIR} for raw signals...")
    
    unique_signals = {}
    
    if not os.path.exists(RAW_DATA_DIR):
        print("‚ùå Workspace folder not found!")
        return

    # Scan for processed text files
    for filename in os.listdir(RAW_DATA_DIR):
        if filename.startswith("Report_") and filename.endswith(".txt"):
            data = parse_report(os.path.join(RAW_DATA_DIR, filename))
            if data:
                unique_signals[data['ticker']] = data
    
    if not unique_signals:
        print("‚ö†Ô∏è No reports found to summarize.")
        return

    signals = list(unique_signals.values())
    signals.sort(key=lambda x: x['prob'], reverse=True)
    
    # Build Markdown Content
    md_output = f"# üèõÔ∏è ARCOS Daily Briefing ({datetime.date.today()})\n\n"
    md_output += "## üöÄ Top Opportunities (Buy Candidates)\n"
    md_output += "| Ticker | Probability | Rationale |\n"
    md_output += "| :--- | :--- | :--- |\n"
    
    buys = [s for s in signals if s['signal'] == "BUY_CANDIDATE"]
    if not buys:
        md_output += "| *None* | - | *No high-conviction signals today* |\n"
    else:
        for s in buys:
            md_output += f"| **{s['ticker']}** | {s['prob']:.2f} | {s['rationale']} |\n"
        
    md_output += "\n## üõ°Ô∏è Risk Alerts (Wait/Sell)\n"
    others = [s for s in signals if s['signal'] != "BUY_CANDIDATE"]
    if not others:
        md_output += "- *No risk alerts active.*\n"
    else:
        for s in others:
            md_output += f"- **{s['ticker']}** ({s['signal']}): {s['rationale']}\n"
        
    md_output += "\n---\n*Generated by ARCOS v1.0 (Logistic Regression Model)*"

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(md_output)
        
    print(f"‚úÖ [Reporter] Briefing generated: {OUTPUT_FILE}")

def run_janitor():
    """
    1. Moves raw files to a daily folder.
    2. Compresses the folder to .zip
    3. Deletes old zips based on RETENTION_DAYS
    """
    today_str = datetime.date.today().isoformat()
    daily_folder = os.path.join(ARCHIVE_DIR, today_str)
    
    # --- A. Move files to daily folder ---
    if not os.path.exists(daily_folder):
        os.makedirs(daily_folder)
    
    moved_count = 0
    for filename in os.listdir(RAW_DATA_DIR):
        if filename.startswith("Report_") and filename.endswith(".txt"):
            shutil.move(os.path.join(RAW_DATA_DIR, filename), os.path.join(daily_folder, filename))
            moved_count += 1
            
    print(f"üßπ [Janitor] Moved {moved_count} files to {daily_folder}")

    # --- B. Compress (Zip) the folder ---
    if moved_count > 0:
        zip_name = os.path.join(ARCHIVE_DIR, today_str)
        shutil.make_archive(zip_name, 'zip', daily_folder)
        
        # Delete the raw uncompressed folder to save space
        shutil.rmtree(daily_folder)
        print(f"üì¶ [Janitor] Compressed archive created: {zip_name}.zip")

    # --- C. Prune old archives (Retention Policy) ---
    print(f"üîç [Janitor] Checking for archives older than {RETENTION_DAYS} days...")
    
    now = time.time()
    cutoff = now - (RETENTION_DAYS * 86400) # 86400 seconds in a day
    
    deleted_count = 0
    for filename in os.listdir(ARCHIVE_DIR):
        filepath = os.path.join(ARCHIVE_DIR, filename)
        
        # Only check .zip files
        if filename.endswith(".zip"):
            file_mod_time = os.path.getmtime(filepath)
            if file_mod_time < cutoff:
                os.remove(filepath)
                deleted_count += 1
                print(f"   üóëÔ∏è Deleted old archive: {filename}")

    if deleted_count == 0:
        print("‚ú® No old archives to prune.")

if __name__ == "__main__":
    generate_briefing()
    run_janitor()